#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Titanicç”Ÿå­˜äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
import warnings
warnings.filterwarnings('ignore')

def preprocess_data(df, is_training=True):
    """ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’è¡Œã†é–¢æ•°"""
    
    # ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ
    df_processed = df.copy()
    
    # 1. å¹´é½¢ã®æ¬ æå€¤ã‚’ä¸­å¤®å€¤ã§è£œå®Œ
    df_processed['Age'].fillna(df_processed['Age'].median(), inplace=True)
    
    # 2. é‹è³ƒã®æ¬ æå€¤ã‚’ä¸­å¤®å€¤ã§è£œå®Œï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
    if 'Fare' in df_processed.columns:
        df_processed['Fare'].fillna(df_processed['Fare'].median(), inplace=True)
    
    # 3. ä¹—èˆ¹æ¸¯ã®æ¬ æå€¤ã‚’æœ€é »å€¤ã§è£œå®Œ
    df_processed['Embarked'].fillna(df_processed['Embarked'].mode()[0], inplace=True)
    
    # 4. å®¢å®¤ç•ªå·ã®æ¬ æå€¤ã‚’å‡¦ç†ï¼ˆæ¬ æã®å ´åˆã¯0ã€ãã†ã§ãªã‘ã‚Œã°1ï¼‰
    df_processed['HasCabin'] = df_processed['Cabin'].notna().astype(int)
    
    # 5. å®¶æ—ã‚µã‚¤ã‚ºã®è¨ˆç®—
    df_processed['FamilySize'] = df_processed['SibSp'] + df_processed['Parch'] + 1
    
    # 6. ä¸€äººæ—…ã‹ã©ã†ã‹ã®åˆ¤å®š
    df_processed['IsAlone'] = (df_processed['FamilySize'] == 1).astype(int)
    
    # 7. å¹´é½¢ã‚°ãƒ«ãƒ¼ãƒ—ã®ä½œæˆ
    df_processed['AgeGroup'] = pd.cut(df_processed['Age'], 
                                      bins=[0, 12, 18, 30, 50, 100], 
                                      labels=[0, 1, 2, 3, 4])
    
    # 8. é‹è³ƒã‚°ãƒ«ãƒ¼ãƒ—ã®ä½œæˆ
    df_processed['FareGroup'] = pd.qcut(df_processed['Fare'], 
                                       q=4, 
                                       labels=[0, 1, 2, 3], 
                                       duplicates='drop')
    
    # 9. æ€§åˆ¥ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    df_processed['Sex_encoded'] = (df_processed['Sex'] == 'female').astype(int)
    
    # 10. ä¹—èˆ¹æ¸¯ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    embarked_mapping = {'S': 0, 'C': 1, 'Q': 2}
    df_processed['Embarked_encoded'] = df_processed['Embarked'].map(embarked_mapping)
    
    # ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã‚’é¸æŠ
    features = ['Pclass', 'Sex_encoded', 'Age', 'SibSp', 'Parch', 'Fare', 
                'Embarked_encoded', 'HasCabin', 'FamilySize', 'IsAlone', 
                'AgeGroup', 'FareGroup']
    
    # æ¬ æå€¤ã‚’å«ã‚€åˆ—ã‚’é™¤å¤–
    available_features = [f for f in features if f in df_processed.columns]
    
    if is_training:
        return df_processed[available_features], df_processed['Survived']
    else:
        return df_processed[available_features]

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    print("ğŸš¢ Titanicç”Ÿå­˜äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’é–‹å§‹ã—ã¾ã™")
    print("="*50)
    
    # 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    train_df = pd.read_csv('Titanic - Machine Learning from Disaster/titanic/train.csv')
    test_df = pd.read_csv('Titanic - Machine Learning from Disaster/titanic/test.csv')
    
    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {train_df.shape}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {test_df.shape}")
    
    # 2. ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†
    print("\nğŸ”§ ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ä¸­...")
    X_train, y_train = preprocess_data(train_df, is_training=True)
    X_test = preprocess_data(test_df, is_training=False)
    
    print(f"å‰å‡¦ç†å¾Œã®ç‰¹å¾´é‡: {X_train.shape}")
    print(f"ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡: {list(X_train.columns)}")
    
    # 3. ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²
    print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ä¸­...")
    X_train_split, X_val, y_train_split, y_val = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
    )
    
    print(f"è¨“ç·´ã‚»ãƒƒãƒˆ: {X_train_split.shape}")
    print(f"æ¤œè¨¼ã‚»ãƒƒãƒˆ: {X_val.shape}")
    
    # 4. è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™
    print("\nğŸ¤– è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­...")
    models = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)
    }
    
    results = {}
    
    for name, model in models.items():
        print(f"\nğŸ” {name} ã‚’è¨“ç·´ä¸­...")
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
        model.fit(X_train_split, y_train_split)
        
        # äºˆæ¸¬
        y_pred = model.predict(X_val)
        
        # ç²¾åº¦ã‚’è¨ˆç®—
        accuracy = accuracy_score(y_val, y_pred)
        results[name] = {
            'model': model,
            'accuracy': accuracy
        }
        
        print(f"ç²¾åº¦: {accuracy:.4f}")
        
        # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        cv_scores = cross_val_score(model, X_train, y_train, cv=5)
        print(f"ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç²¾åº¦: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
    
    # 5. æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])
    best_model = results[best_model_name]['model']
    best_accuracy = results[best_model_name]['accuracy']
    
    print(f"\nğŸ† æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«: {best_model_name}")
    print(f"ç²¾åº¦: {best_accuracy:.4f}")
    
    # 6. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
    print("\nğŸ”® ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬ä¸­...")
    predictions = best_model.predict(X_test)
    
    print(f"äºˆæ¸¬å®Œäº†: {len(predictions)}ä»¶")
    print(f"ç”Ÿå­˜äºˆæ¸¬: {predictions.sum()}äºº")
    print(f"æ­»äº¡äºˆæ¸¬: {(predictions == 0).sum()}äºº")
    print(f"ç”Ÿå­˜ç‡äºˆæ¸¬: {predictions.mean():.3f} ({predictions.mean()*100:.1f}%)")
    
    # 7. çµæœã®ä¿å­˜
    print("\nğŸ’¾ çµæœã‚’ä¿å­˜ä¸­...")
    test_df_with_predictions = test_df.copy()
    test_df_with_predictions['Survived'] = predictions
    
    output_filename = f'titanic_predictions_{best_model_name.replace(" ", "_").lower()}.csv'
    test_df_with_predictions.to_csv(output_filename, index=False)
    
    print(f"äºˆæ¸¬çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_filename}")
    
    # 8. ç‰¹å¾´é‡ã®é‡è¦åº¦ï¼ˆRandom Forestã®å ´åˆï¼‰
    if isinstance(best_model, RandomForestClassifier):
        print("\nğŸ“Š ç‰¹å¾´é‡ã®é‡è¦åº¦:")
        feature_importance = pd.DataFrame({
            'feature': X_train.columns,
            'importance': best_model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        for _, row in feature_importance.head(10).iterrows():
            print(f"  {row['feature']}: {row['importance']:.4f}")
    
    # 9. ãƒ‡ãƒ¼ã‚¿ã®æ„å‘³ã‚’è¡¨ç¤º
    print("\nğŸ“‹ ãƒ‡ãƒ¼ã‚¿ã®æ„å‘³:")
    print("â€¢ Survived: 0=æ­»äº¡, 1=ç”Ÿå­˜")
    print("â€¢ Pclass: 1=1ç­‰å®¢å®¤, 2=2ç­‰å®¢å®¤, 3=3ç­‰å®¢å®¤")
    print("â€¢ Embarked: C=ã‚·ã‚§ãƒ«ãƒ–ãƒ¼ãƒ«, Q=ã‚¯ã‚¤ãƒ¼ãƒ³ã‚ºã‚¿ã‚¦ãƒ³, S=ã‚µã‚¦ã‚µãƒ³ãƒ—ãƒˆãƒ³")
    print("â€¢ Sex: male=ç”·æ€§, female=å¥³æ€§")
    print("â€¢ Age: å¹´é½¢")
    print("â€¢ SibSp: å…„å¼Ÿå§‰å¦¹ãƒ»é…å¶è€…ã®æ•°")
    print("â€¢ Parch: è¦ªãƒ»å­ä¾›ã®æ•°")
    print("â€¢ Fare: é‹è³ƒ")
    print("â€¢ Cabin: å®¢å®¤ç•ªå·")
    print("â€¢ Ticket: ãƒã‚±ãƒƒãƒˆç•ªå·")
    
    print("\nğŸ‰ å®Œäº†ã—ã¾ã—ãŸï¼")

if __name__ == "__main__":
    main() 